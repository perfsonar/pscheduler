## Creating and Understanding a pScheduler Test


### 1. Understanding pScheduler Tests

Before writing a test, it's important to understand how tests are used in pScheduler as well as the perfSONAR 
best practices for developers. The wiki (https://github.com/perfsonar/project/wiki) contains help on how to develop for perfSONAR
in general. Examples of tests can be found in the pScheduler source code (anything with test in the folder name in the 
main directory is a test). 

### 2. Running the PDK setup script

Once you understand what you want to accomplish with your test, you'll want to make sure you have a pScheduler development 
environment set up on your development machine. The instructions for how to do this can be found on the general README page for 
the pScheduler repository. Then, you'll want to run the plugin_dev script as specified in the PDK README. This script will set 
up all of the files you need for your test and fill in the boilerplate code needed for a basic perfSONAR test. You
may also want to run the make commands indicated in the PDK README to make sure that everything is ready to go out of the box.

### 3. Developing your test

After the files are generated, you're ready to begin developing! Below we have a more thorough explanation of all of the files 
and directories generated by the plugin_dev script, which may be helpful to read through before you begin writing code.

### 4. Testing your test
There are two main ways to test your test:

1. Testing with scheduled pScheduler tasks or

2. Testing with premade JSON files

It's important to utilize both means of testing in your development workflow. However, for debugging purposes, the second 
testing method is usually more useful in terms of output and is also much faster.

#### Method 1:

-Follow the pScheduler documentation on how to use a test (https://docs.perfsonar.net/pscheduler_ref_tests_tools.html)

#### Method 2:

_This method is somewhat more involved to set up, but ultimately it will allow you to debug much easier. If you do not run 
your test in this way, you will **not** be able to see any print statements you generate in your test code. **This 
includes error messages!** Running your test in the regular scheduled test format is important to verify that it works 
in that manner because that is how it will be used "in the wild", however, it will "fail silently" when run that way which won't 
help you make progress in developing it._

1. Short, direct piping

Each formatting file (cli-to-spec, spec-to-cli, etc.) can take direct pipe input, and you can also chain these files together. For example, you can test cli-to-spec by doing the following:

```./cli-to-spec < --my-option-name my-argument```

This will allow you to make sure that your required arguments are being enforced correctly. The output of this will be the spec json, which you can then pipe into spec-format for example, to make sure that the plaintext and html representations of the spec look good. An example of how to do that is here:

```cat spec-example.json | ./spec-format text/plain```

Where ```spec-example.json``` is spec json generated through cli-to-spec.

2. More complicated JSON editing

A similar process can be used to test result-format. An example result file is provided by the PDK, however this will need to be edited for your specific test to work. You'll want to edit the "result" portion of the json (assuming your tool will insert it's results into this section of the json). There are two ways to do this:

- If you have already written your tool, the easiest way to do it is to simply generate result json with the tool and replace the example-result file provided by the PDK with this json.

- If you have not yet written the tool (or it's not in a state to generate output json), you can instead edit the provided example json to meet the requirements of your test. Simply insert json dictionary objects into the result section of the json.

Once you have this json, you can pipe it into result-format using the same process as specified for spec format (there's also direct instructions at the top of the result-format file). You can use this to test that your required options for the result are being correctly enforced and being outputted correctly. 

### 5. Debugging your test

Once you're successfully getting good output, it's time to run your test with pScheduler. In order to do this, you will need your tool to be working as well. First, you'll need to do a ```make cbic``` in both the test and tool directories. You'll need to run a ```make cbic``` in your test directory any time you make a change to your test or the change won't be reflected when you run a pScheduler task.

You'll also need to add both the test and the tool to the pScheduler RPM file. This can be found at https://github.com/perfsonar/pscheduler/blob/master/scripts/RPM-BUILD-ORDER.m4 (navigate to directory ```pscheduler/scripts/``` and open ```RPM-BUILD-ORDER.m4```. Any packages (including python packages) or external tools you added to pScheduler to run your test/tool will need to be added to this file in the appropriate section (the section for these is above where you will add your test/tool). Then, you'll need to add the test and tool names in the appropriate sections. Be very careful to spell everything correctly or pScheduler will not recognize your test/tool. This enables pScheduler to make your test and tool during a pScheduler make and adds them to pScheduler. You can then run a make on pScheduler.

## Anatomy of a Test

### enumerate

This is the first file you want to edit when developing your test. Not much needs to be done to complete this file, most of it will be filled in when you run the PDK to create your test. You will need to add a brief description of your test. The description does not need to be long, it just needs to have enough information to make it clear what the test is for and what it does. You'll also want to verify the information filled in by the PDK to make sure it is accurate. If you ever need to change the name of your test, make sure to update this file with the correct information.

### validate.py

This file represents a significant portion of the development work for a test. It defines both the validity of the input spec and the results format. The JSON types used in this file can be found here: https://github.com/perfsonar/pscheduler/blob/master/python-pscheduler/pscheduler/pscheduler/jsonval.py as they are pScheduler specific. Most of the time, you will want to preserve the properties in this file that are given to you in the template. However, they may not be required so make sure to modify that aspect if needed. Required properties are the inputs absolutely needed to run the test; they need to have values specified at the time the test is run. Without them, the test and tool will not have enough information to run and will fail. Generally, try to make as few properties as possible required properties. You can add default values to allow a test to be run without a certain property having a value specified. If the underlying tool offers defaults itself, you also can rely on those when considering if a property needs to be required. Typically, limit_is_valid does not need to be edited after the PDK is run.

### cli-to-spec

This file also represents a significant portion of the development work for a test. Most of the work in this file is adding in the options that the test can use to run and then processing which options are required and adding defaults for ones that are not and that the tool does not provide built-in defaults for. When adding options, make sure to follow the same format as the options provided in the PDK template. The name of the option should match the property you specified in validate.py. The help section should be a brief string describing the property and it's helpful to include the formatting you're expecting for the option when the test is being invoked. This string will appear if a user neglects to provide a required option or incorrectly invokes any option. The type of an option will typically be a string or a number, make sure to set an appropriate type for your argument. There needs to be an option in this file (in the schema section, not the results section) for every property you specified in validate.py, whether the option is required according to validate.py or not. The options given by the PDK template show how to check if an option was given on the command line and how to cause a fail state if a required option is missing or an option has values that aren't accepted. If you do not need an option to run the test, you can simply check to see if the value is None and there is no need for an else statement. If you need to act based on the missing information, either because it's a required option with no value given or because you need to insert a default value, you'll need to add an else statement and take the correct action there. The testtype option from the PDK template provides a good example of how to check to see if the value given for an option falls in a list of expected values. Typically, if an unexpected value is given, you'll want to fail instead of providing a default in case the user provided a meaningful option that simply had a typo or was formatted incorrectly. There's a variety of ways for options to not be acceptable based on what's expected. Values may need to fall in a certain range or have a certain prefix or postfix, etc. There's lots of ways to check these things because the syntax used here is pure Python 3. It's up to you to enforce how required options will be dealt with here, so make sure to address all of them appropriately. Generally, you do not want to edit any of the other code provided by the template.

### spec-to-cli

This file requires some relatively minor edits. You'll need to add tuples for all your arguments in the same format as the default arguments provided by the PDK template. These are input arguments only. Make sure to include ALL arguments, whether they are required or not.

### spec-is-valid

You most likely will not need to edit this file after it is generated by the PDK. It checks to see if a test spec is valid. You do your specification of validity in other files so that this file can successfully do it's job without direct edits.

### participants

You most likely will not need to edit this file after it is generated by the PDK. It outputs the list of the hostnames/IP addresses participating in the test.

### result-format

This file formats the result of the test in both html and plain text format. This file will probably require a significant amount of development work. All of the data specified in validate.py as being a part of the result will need to be presented here in a human readable way. The plain text is a python printout. A bit of an example is provided by the PDK template and you can also look at pre-existing tests to get a feel for what output usually looks like and how it is formatted. Your actual output will be very specific to your individual tests but the formatting for pScheduler test output tends to be very similar. The html output should contain everything the plain text output does, but it will be formatted to be a well-organized html webpage. Pre-existing plugins should also have good examples of this. It doesn't need to be fancy, but a basic knowledge of html will probably be necessary to complete this.

### spec format

This file is very similar to result-format but it operates on the spec instead of the result. It formats the spec JSON into something human readable in both plain text and html format. You can look at pre-existing plugins here as well to see how the formatting of the spec usually is arranged.
