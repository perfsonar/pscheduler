#!/usr/bin/env python3

#
# Development Order #3:
# 
# This file will determine if this tool can run a test based on a test spec.
#
# Be sure to edit line 19, inserting the names of the tests the tool
# should be compatible with.
# 

# exit statuses should be different based on error

import pscheduler

json = pscheduler.json_load(exit_on_error=True);


try:
    test_type = json["type"]
except KeyError:
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": [ "Missing test type" ]
    })


if test_type == "disk-to-disk":
    max_schema = 1
elif test_type == "http":
    max_schema = 4
else:
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": [ "Unsupported test type" ]
    })



try:
    pscheduler.json_check_schema(json["spec"], max_schema)
except ValueError as ex:
    # Schema check failed
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": [str(ex)]
    })
except KeyError:
    # Data was missing
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": ["Missing data in test specification"]
    })

if test_type == "http" \
   and json["spec"]["url"][0:5] == "file:" \
   and json["spec"].get("success-content", False):
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": ["Cannot return content for file:// URLs"]
    })
  
    

pscheduler.succeed_json({ "can-run": True })
