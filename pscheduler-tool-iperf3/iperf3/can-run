#!/usr/bin/python
#
# Determine if this tool can run a test based on a test spec.
#

import datetime
import sys

import pscheduler

logger = pscheduler.Log(prefix='tool-iperf3', quiet=True)

json = pscheduler.json_load(exit_on_error=True)

logger.debug("can-run for %s" % json)

try:
    if json['type'] != 'throughput':
        pscheduler.succeed_json({
            "can-run": False,
            "reasons": [ "Unsupported test type" ]
        })
except KeyError:
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": [ "Missing test type" ]
    })


try:
    spec = json["spec"]
    pscheduler.json_check_schema(spec, 2)
except KeyError:
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": ["Missing test specification"]
    })
except ValueError as ex:
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": [str(ex)]
    })


errors = []

supported_options = ["schema",
                     "source", "source-node",
                     "dest", "dest-node",
                     "reverse", "omit", "single-ended", "single-ended-port",
                     "duration", "interval", "parallel", "window-size",
                     "mss", "bandwidth", "udp", "buffer-length",
                     "ip-tos", "local-address", "ip-version", "congestion",
                     "zero-copy", "flow-label", "client-cpu-affinity",
                     "server-cpu-affinity"
                     ]

for option in spec.keys():
    if option not in supported_options:
        logger.debug("iperf3 unsupported option %s" % option)
        errors.append('iperf3 does not support %s option' % option)    

logger.debug("can-run succeeded")

result = {
    "can-run": len(errors) == 0
}

if len(errors) > 0:
    result["reasons"] = errors

pscheduler.succeed_json(result)
