%
% pScheduler Developer's Guide
%

% TODO: Add titlepage to the options.
\documentclass[10pt,titlepage]{article}

\input pscheduler-tex.tex

\DRAFT

\title{pScheduler Plug-In Developer's Guide}
\author{The perfSONAR Development Team}


\begin{document}
\maketitle
\tableofcontents


\part{Introduction}

\section{Developing for pScheduler}

This guide provides the information required to build software modules
that add to pScheduler's capabilities.

\todo{Expand this.}


\section{Configuring a System for Development and Test}

Development and test is currently supported on Red Hat Enterprise
Linux or CentOS 6.{\it x}.

\todo{Is the model going to be that we develop on CentOS and do ports to Debian?}


\subsection{System Requirements}

The minimum recommended configuration for build and development is as
follows:

\begin{itemize}
\item 32- or 64-bit Intel CPU.  (64-bit recommended)
\item 2 GiB DRAM
\item 10 GB disk
\item Network connection which can reach the Internet, specifically
  the PostgreSQL YUM repository at {\tt download.postgresql.org} and a
  RHEL or CentOS mirror.  (The perfSONAR development team will
  eventually make this repository available to sites unable to access
  non-R\&E networks.)
% TODO: Make PgSQL available on software.internet2.edu.
\end{itemize}

\subsection{Installation and Configuration}

Follow these steps to configure the build/development system:

\begin{enumerate}
\item Boot the Red Hat Enterprise Linux or CentOS 6.{\it x} or 7.{\it
  x} image.
\item Configure the language, keyboard and storage devices
  appropriately.
\item Configure the network interface(s) to connect automatically.
\item Configure the time, root password, and disk usage appropriately.
\item Use the ``Minimal'' configuration for installation.
\item Finish the installation and reboot.
\item Log in as \root.
\item On CentOS6, \rootcommand{yum -y install openssh-clients}
\item \rootcommand{yum -y upgrade}
\item Reboot.
\item Copy the file {\tt scripts/system-prep} from the pScheduler
  source distribution onto the new machine and place it on the new
  system in {\tt /tmp}.
\item If the system is a virtual machine hosted on VirtualBox on Linux
  or OS X and you wish to have your account and home directory
  available, configure the guest with a shared folder for your home
  directory and named to match your login (e.g., {\tt /home/bob} as
  {\tt bob}).  Then edit {\tt system-prep}, uncomment and set the four
  environment variables at the top and attach the {\it VirtualBox
    Guest Additions} CD image.  \todo{How do we handle a CD-only
    install, where the Guest Additions image has to be in one drive
    and the CentOS image has to be in the other?  Also need access to
    the PostgreSQL repo, which needs network.}
\item \rootcommand{/tmp/system-prep}
\end{enumerate}


If the system is a VirtualBox VM and you intend to use a shared folder
for development, the VM must have the ability to create symbolic links
on those filesystems.  This requires at least VirtualBox verion 5.0.5
and can be enabled by shutting down the guest and executing executing
the following command on the host:

\begin{lstlisting}[language=json,firstnumber=1]
VBoxManage setextradata VM-NAME \
    VBoxInternal2/SharedFoldersEnableSymlinksCreate/SHARE-NAME 1
\end{lstlisting}


If the system is a virtual machine, this is a good point to create a
snapshot or export it as an appliance.


\subsection{Building and Installing pScheduler}

pScheduler is divided into multiple packages, some of which have
dependencies on others and each must be installed after being built.
You should, therefore, consider the build/development system
expendable and not do development on a production system.

Other than the installation of finished packages, the build process
does not produce by-products outside of the source directories.

Once you have a system configured as described above, build and
install pScheduler by doing the following:

\begin{enumerate}
\item Copy the pScheduler source distribution onto the system.
\item Become \root.
\item Unpack the source distribution.
\item \rootcommand{cd {\it pScheduler source directory}}
\item \rootcommand{make}
\end{enumerate}

Everything needed for pScheduler will be started on the next system
boot.

The effects of the installation can be un-done with \command{make
  uninstall}.


% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------


\part{The pScheduler Plug-In Architecture}

As its name hints, pScheduler's job is to do five things, all of which
center around making measurements:

\begin{enumerate}
\item Accept tasking from the outside.
\item Maintain a schedule of one or more times when the tasking should
  be carried out.  These events are called {\it runs}.
\item Carry out the tasking by executing the scheduled runs.
\item Receive run results.
\item Distribute the results to archives for processing and long-term
  storage.
\end{enumerate}

One of the key tenets of pScheduler's design is that it should have no
understanding of the measurements it schedules or the programs which
to carry them out.  This is accomplished through abstraction and a
system of plug-ins which can be installed in any combination on a
perfSONAR system.  By using plug-ins, perfSONAR's capabilities can be
expanded by its user community without having to rely on the core
development team's limited resrouces.  Software to handle measurements
not of broad enough interest to be part of the mainstream perfSONAR
distribution can be developed and maintained by those who need them.

In order to make this happen, pScheduler establishes three classes of
things with which it can interface:

\begin{itemize}
\item {\it Tests}, which are abstract descriptions of a measurement to
  be made and the results.
\item {\it Tools}, which are programs which, in single or multiple
  instances working together, make a measurement specified by a test.
\item {\it Archivers}, which send test results produced by tools
  elsewhere for long-term storage.
\end{itemize}

Each of these will be described in more detail in later sections.

\section{Concept of Operation}

\todo{Introductory paragraph}

This is the lifecycle of a measurement:

\begin{enumerate}
\item On startup, pScheduler inventories the installed tests, tools
  and archivers.  For tools, this includes getting information on
  which tests they run.
\item A task, consisting of a test specification, scheduling,
  archiving and other administrative information is written and
  provided to pScheduler.
\item The test specfication is validated by the test whose type is
  specified in the task.
\item The test specification is given to the test, which uses it to
  determine the list of nodes that will be participating.
\item pScheduler provides the test to each of the participants and is
  returned a list of the tools which are able and willing to carry out
  the test in order of preference.
\item The lead participant combs through the lists and selects a tool
  to be used for runs of the task.
\item The task is registered with each of the participants.
\item One or more runs of the task are scheduled.  As part of the
  scheduling process, the tool on each participant generates
  participant-specific data for that run, such as ports to be used.
\item The participant-specific data is collected, merged and
  distributed to all participants.
\item At the scheduled time, the measurement is performed, producing a
  result which is stored locally by each participant.
\item The lead participant retrieves and stores the other
  participants' results.
\item The set of participant results are merged into a full result,
  which is stored by the lead participant.
\item The lead participant puts the results in a queue to be archived.
\item The results are sent to the specified archives, repeatedly if
  necessary because of failures.
\end{enumerate}


\section{Plug-In Concepts}

\subsection{Classes}

At the top level of the API are {\it classes}, each of which defines
an entity pScheduler requires to function.

There are three defined classes, which are the three items described
earlier (tests, tools and archivers).

\subsection{Methods}

Each class defines one or more {\it methods}, which carry out
functions specific to that class.

Methods are invoked across a process boundary (i.e., by running
programs external to pScheduler).  This almost completely decouples
pScheduler's technology stack from those of the components, providing
several benefits:

\begin{itemize}
\item Developers are free to select the right tool for the job 

developers to select the right tool for the job and allowing
significant evolution of one part without requiring the same of the
others.
\end{itemize}

Each method program will:

\begin{itemize}
\item Be installed in a defined location.
\item Have a defined name based on its function.
\item Accept input in a defined format.  For most programs, this will
  be JSON data.
\item Provide output on the standard output stream in a defined
  format.  For most programs, this will be JSON data.
\item Provide error output on the standard error stream as plain,
  human-readable text.
\item Exit with one of several defined status values, usually
  following the \true/\false\ convention or using values in
  the \headerfile{sysexits.h} header found on many systems.
\end{itemize}


\subsection{Implementations}
\todo{Write this.  (What was I going to put in this section?)}


\subsection{Directory Structure}

\note{This information is provided as background and should not be
  used in determining where files should be installed.  Specific
  instructions for use with the standard packaging systems for various
  OS distributions can be found later in this document.}

Generally, pScheduler's installation of files follows the conventions
put forth in the {\it Filesystem Hierarchy Standard}
(\url{http://www.pathname.com/fhs}).

A typical directory tree on a system with several tests, tools and
archivers installed might look like this:

\dirtree{%
.1 /usr.
.2 libexec.
.3 pscheduler.
.4 classes $\odot$.
.5 test $\odot$ {\it Implementations of tests}.
.6 idle.
.6 rtt.
.5 tool $\odot$ {\it Implementations of tools}.
.6 sleep.
.6 snooze.
.6 ping.
.6 owping.
.5 archiver $\odot$ {\it Implementations of archivers}.
.6 bitbucket.
.6 esmond.
.2 share.
.3 doc.
.4 pscheduler.
.5 test $\odot$ {\it Test documentation}.
.5 tool $\odot$ {\it Tool documentation}.
.5 archiver $\odot$ {\it Archiver documentation}.
}

The full paths of directories marked with the symbol $\odot$ should be
provided as macros for packaging system specifications to use in
ensuring correct placement of installed files.

\todo{Should we specify the name of a directory inside each
  test/tool/archiver where private code can live?  (Thinking about
  common JSON validator modules which would to be used for tool {\tt
    can-run} and {\tt run} methods.)  {\tt PRIVATE} might be a good
  candidate.}


% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------


\section{Plug-In Classes}

\todo{Need to add a ``Command Line Arguments'' item to the description
  of each method.}

\todo{Change the ``Exit Status'' sections to be enumerated so they're easier to read.}

%
% TEST
%
\subsection{Test}

\todo{Introductory material}

\todo{Design of Tests}

\todo{Work this in somewhere: Tests are not necessarily restricted to
  the subset of parameters the tools which implement it have in
  common, rather they should be the superset of all parameters.  This
  makes it possible to use tool-unique features without requiring that
  every tool understand every parameter.  Tools will seeing tests
  containing parameters they do not understand or are outside of their
  abilities will simply decline to perform them.}

\todo{Test Specification}

The specification for a round-trip time test might look like this:
\begin{lstlisting}[language=json,firstnumber=1]
{
  "schema": 1,
  "source": null,
  "target": "somehost.example.org",
  "duration": "PT20S",
  "interval": "PT3S",
  "size": 1024,
  "ttl": 100,
  "tos": "0x5a"
  }
}
\end{lstlisting}



Every test must implement the methods listed in the following sections.

The directory tree of a test implementation would look like this ({\tt
  *} denotes an executable file):
\dirtree{%
.1 /usr/libexec/pscheduler/classes/test.
.2 {\itt test-name}.
.3 cli-to-spec*.
.3 enumerate*.
.3 participants*.
.3 result-format*.
.3 result-is-valid*.
.3 spec-is-valid*.
.3 spec-to-cli*.
}


\todo{Put these in alphabetical order.}

\subsubsection{enumerate}

This method produces a description of the test in a standard format
used when pScheduler needs to determine what tests are installed.

\calloutitem{Standard Input} Ignored.

\calloutitem{Standard Output} JSON containing the following items:
\begin{itemize}
\item{\tt name} - A string serving as a short name for the test.  This
  value should be lowercase and match the name of the directory where
  the test class is installed.  Test names should be industry-standard
  terms rather than the names of the tools which carry them out.  For
  example, a test that determines how long it takes for packets to get
  from one host to another should be called {\tt latency} and not {\tt
    owamp}.  A test the determines the same for a round trip should be
  called {\tt rtt} (short for {\it round-trip time}) and not {\tt
    ping}.
\item{\tt description} - A short, textual description of what the test
  measures.
\item{\tt version} - A \jsontype{Version}.
\item{\tt maintainer} - A \jsontype{Maintainer}.
\end{itemize}

\example
\begin{lstlisting}[language=json,firstnumber=1]
{
    "name": "rtt",
    "description": "Round-trip time between hosts",
    "version": "1.0",
    "maintainer": {
        "name": "Example Development Team",
        "email": "plug-ins@example.org",
        "href": "http://www.example.org/plug-ins"
    }
}
\end{lstlisting}

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if successful, nonzero on error.

% TODO: At some point in the future, there will probably be an
% enumeration of what the test itself should look like for use by
% GUIs.


\subsubsection{participants}
This method produces a list of the systems participating in the test
based on the test specification.

\calloutitem{Standard Input} A JSON specification of the test.

\calloutitem{Standard Output} A JSON array of the participants, with
any {\tt null} values indicating the lead participant.  The order of
this list will match the numbering of participant roles specified in
the definition of the test.

\example
\begin{lstlisting}[language=json,firstnumber=1]
[ null, "receivinghost.example.com" ]
\end{lstlisting}

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if successful, nonzero on error.



\subsubsection{spec-is-valid}
This method determines whether not a test specification is valid.

Validation of a parameter should be according to whether or not its
value is usable in a test, not whether or not any of the tools
actually implement it.  For example, a test with a {\tt port}
parameter representing a UDP or TCP port should accept numeric values
in the range {\tt 0} to {\tt 65535} and perhaps strings which can be
translated to port numbers using {\tt getservbyname(3)}.  Tools will
accept or decline individual tests based on their own restrictions.

\calloutitem{Standard Input} A JSON test specification.

\calloutitem{Standard Output} None.

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if the test specification was valid,
            {\tt 1} if not, {\tt 2} if there was some other problem
            unrelated to the input.



\subsubsection{result-is-valid}
This method determines whether or not a result produced by a tool for
this test conforms to the expected format.  \todo{None of the tests so
  far have this.  Not sure there's a need for it.}

\calloutitem{Standard Input} JSON results for the test as produced by
a tool.

\calloutitem{Standard Output} None.

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if the result was valid, {\tt 1} if
not, {\tt 2} if there was some other problem unrelated to the input.



\subsubsection{cli-to-spec}
This method produces a test specification from a set of command-line
switches which, when fed to {\tt spec-to-cli} will produce the same
set of command-line switches.

This method can be invoked as a filter by providing a JSON array of
arguments on the standard input or as a converter by providing the
arguments on the command line.  The set of valid arguments is
test-specific.

\calloutitem{Standard Input} None if arguments are present on the
command line, otherwise a JSON array of strings, each being a
command-line argument.

\calloutitem{Standard Output} A JSON specification for the test.

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if the conversion was a success,
            {\tt 1} if not, {\tt 2} if there was some other problem
            unrelated to the input.



\subsubsection{spec-to-cli}
This method produces a list of command-line switches from a test
specification which, when fed to {\tt cli-to-spec} will produce the
same specification.

\calloutitem{Standard Input} A specification valid for the test.

\calloutitem{Standard Output} A JSON array of strings, each having
being a command-line argument suitable for passing to {\tt
  cli-to-spec}.

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if the conversion was a success,
            {\tt 1} if not, {\tt 2} if there was some other problem
            unrelated to the input.


\subsubsection{result-format}
This method converts a JSON result for this test into some other
format, usually intended for human consumption.  Note that
implementations of this method must be able to format output for a
failed test, normally a short ``test failed'' message.

\calloutitem{Arguments} The desired MIME type for the output.  All
implementations must support at least {\tt text/plain} and {\tt
  text/html}.

\calloutitem{Standard Input} JSON containing a result for this test.

\calloutitem{Standard Output} The formatted output.

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} on success, {\tt 1} if not, {\tt 2}
if there was some other problem unrelated to the input.


\subsubsection{specformat}
This method converts a JSON specification for this test into some
other format, usually intended for human consumption.

\calloutitem{Arguments} The desired MIME type for the output.  All
implementations must support at least {\tt text/plain} and {\tt
  text/html}.  If no argument is provided, the default will be {\tt
  text/plain}.

\calloutitem{Standard Input} JSON containing a specification for this
test.

\calloutitem{Standard Output} The formatted output.

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} on success, {\tt 1} if not, {\tt 2}
if there was some other problem unrelated to the input.


\subsubsection{limit-is-valid}
This method validates a limit specification for the test.

\calloutitem{Arguments} None

\calloutitem{Standard Input} JSON containing the limit specification
to be validated.

\calloutitem{Standard Output} A JSON object containing the following pairs:

\todo{This is duplicated from above; should be part of the JSON
  dictionary.}
\begin{itemize}
\item \typeditem{valid}{Boolean} An indication of whether or not the
  input was valid.

\item \typeditem{error}{String} if {\tt valid} was \false, a plain
  text message indicating the reason.
\end{itemize}

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} on success, {\tt 1} if there was a
problem unrelated to the input.



\subsubsection{limit-passes}

This method checks to see if a test specification falls within the
restrictions of a limit.

\calloutitem{Arguments} None

\calloutitem{Standard Input} A JSON object containing the following
pairs:

\begin{itemize}
\item \typeditem{spec}{AnyJSON} The specification of the test parameters.
This must be valid for the current test type.

\item \typeditem{limit}{AnyJSON} The limit to be checked.  This must be
valid for the current test type.

\end{itemize}


\calloutitem{Standard Output} A JSON object containing the following pairs:

\begin{itemize}
\item \typeditem{passed}{Boolean} An indication of whether or not the
  test was within the limit's constraints.

\item \typeditem{errors}{Array} if {\tt valid} was \false, an array of
  strings containing text messages enumerating the reasons for the
  failure.
\end{itemize}

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} on success, {\tt 1} if there was a
problem unrelated to the input.



%
% FEATURE
%
\subsection{Feature}

\todo{This feature will not be present in early releases.}

\todo{Write this.  Features will be what take care of altering the
  environment using chains of exec'd programs prior to running a tool
  and tearing down afterward.  This is to solve the MDVPN problem.}

\todo{Consider a different name for these.  Modifiers?  Preparers?}



%
% TOOL
%
\subsection{Tool}
\todo{Write this.}

\subsubsection{enumerate}

This method produces a description of the tool in a standard format
used when pScheduler needs to determine what tools are installed.

\calloutitem{Standard Input} Ignored.

\calloutitem{Standard Output} JSON containing the following items:
\begin{itemize}
\item{\tt name} - A string serving as a short name for the tool.  This
  value should be lowercase and match the name of the directory where
  the tool class is installed.  The name should reflect the software
  used, such as {\tt traceroute}, {\tt tracepath} or {\tt mtr}.
\item{\tt description} - A short, textual description of what the tool
  does.
\item{\tt version} - A \jsontype{Version}.
\item{\tt tests} - An array of strings listing the tests which the
  tool is capable of accommodating.  Including a class here does not
  imply that the tool is required to actually perform the test, only
  that it should be asked if it can perform tests of those listed.
\item{\tt preference} - An integer used to determine the order in
  which tools are selected for tests, with larger numbers indicating
  more oreference.  This allows current, better-supported tools to be
  considered first while still allowing those that are deprecated to
  be used if necessary.  Nominally, the value for most tools should be
  {\tt 0} and deprecated tools should use a negative number.  This
  value is considered advisory only, as some measurements may specify
  that a specific tool be used to conduct a measurement.
\item{\tt maintainer} - A \jsontype{Maintainer}.
\end{itemize}

\example
\begin{lstlisting}[language=json,firstnumber=1]
{
    "name": "mtr",
    "description": "Determine network path using MTR",
    "version": "1.0",
    "tests": [ "path" ],
    "preference": 0,
    "maintainer": {
        "name": "Example Development Team",
        "email": "plug-ins@example.org",
        "href": "http://www.example.org/plug-ins"
    }
}
\end{lstlisting}

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if successful, nonzero on error.



\subsubsection{can-run}
This method determines whether or not the tool can run a specified test.  The test must be 

\calloutitem{Standard Input} A \jsontype{TestSpecification}

\calloutitem{Standard Output} None.

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if the tool is willing to run the
specified test, {\tt 1} if not, {\tt 2} if there was an error related
to the input and {\tt 3} for an error unrelated to the input.



\subsubsection{duration}

This method determines how long the tool will require to run the
specified test.  This figure should include all time required to set
up before the test and tear down afterward.

\calloutitem{Standard Input} A \jsontype{TestSpecification}.

\calloutitem{Standard Output} The amount of time required, specified
as a \jsontype{Duration}.  This value should be in whole seconds; any
fraction will be increased to the next-largest number of seconds
(e.g., {\tt PT7.13S} will be converted to {\tt PT8S}).

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if determination of the duration was
successful, {\tt 1} if not, {\tt 2} if there was an error related to
the input and {\tt 3} for an error unrelated to the input.



\subsubsection{participant-data}

This method returns a tool-specific set of data for a given
participant in a given test.  This data will be gathered centrally and
distributed to all participants, who can use it in carrying out the test.

The canonical use case for this is a tool which requires that one
participant open a port and another connect to it.  Listening
participant 1 would put the port it intends to use in its participant
data.  Connecting participant 0 would use that information in
establishing its connection.


\calloutitem{Standard Input} A JSON object containing the following items:

\begin{itemize}
\item{\tt test} - A \jsontype{TestSpecification}.

\item{\tt participant} - The number of the participant whose data
  should be generated.  Must be in the range of {\tt 0} to the number
  of participants
\end{itemize}


\calloutitem{Standard Output} An \jsontype{AnyJSON} containing
whatever the tool deems necessary to provide.  If the tool has nothing
to contribute for a given participant, the object should be empty
(i.e., {\tt \{\}}).

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if generation of the participant
data was successful, {\tt 1} if not, {\tt 2} if there was an error
related to the input and {\tt 3} for an error unrelated to the input.



\subsubsection{run}
This method carries out the test as one of its participants.
\todo{Could use some examples.}

\calloutitem{Standard Input} A JSON run specification containing the following items:
\begin{itemize}

\item{\tt schema} - A \jsontype{Cardinal} indicating the version of the structure

\item{\tt schedule} - A \jsontype{TimeInterval} indicating when the
  task is scheduled to start and how long it should run.  This figure
  includes any additional time added by the tool for startup and
  teardown, so all participants should behave accordingly.

\item{\tt test} - A \jsontype{TestSpecification} describing the test
  being run.

\item{\tt participant} - A \jsontype{Number} indicating which
  participant role in the test is to be played.

\item{\tt participant-data} - A JSON array containing one element of
  tool-specific, per-participant data for the run of the task, as
  generated by the {\tt participant-data} method.

\end{itemize}

\calloutitem{Standard Output} A single JSON object containing the following:

\begin{itemize}
\item{\tt succeeded} - A \jsontype{Boolean} indicating whether or not
  the run was a success.

\item{\tt diags} - A free-form \jsontype{String} containing diagnostic
  output.  This is usually populated with raw program output or other
  information the tool finds useful to provide.

\item{\tt error} - A free-form \jsontype{String} containing any errors
  encountered by the tool while running.  This should be {\tt null} or
  empty if the test succeeded.  Non-fatal warnings should be
  incorporated into {\tt diags}.

\item{\tt result} - A JSON object containing test-specific results for
  this participant.

\end{itemize}

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if the test ran successfully, {\tt
  1} if there was a test-related error (i.e., the test could run but
was unable to produce results because a remote participant failed to
answer) or {\tt 2} if there was an error unrelated to the test.



\subsubsection{merged-results}

This method takes the results from all participants of a run done with
this tool and prodces a single, merged result in the format specified
by the test being conducted.

\calloutitem{Standard Input} A single JSON object containing the following:

\begin{itemize}

\item{\tt test} - A \jsontype{TestSpecification} describing the test
  that was run.

\item{\tt results} - A JSON array containing one test result for each
  participant in the test.  Each is any valid JSON object and the
  format is tool-specific.

\end{itemize}

\calloutitem{Standard Output} A single JSON object representing the
result.  This object must contain at least the following:

\begin{itemize}

\item{\tt succeeded} - A \jsontype{Boolean} indicating whether the
  test was a success or failure.

\end{itemize}


\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if generation of the merged result
was successful, {\tt 1} if there was an error related to the input or
{\tt 2} if there was an error unrelated to the input.



%
% ARCHIVER
%
\subsection{Archiver}
Archivers are plug-ins which transfer the results of a run to
long-term storage.

\todo{Flesh this out.}


\subsubsection{enumerate}

This method produces a description of the archiver in a standard
format used when pScheduler needs to determine what archivers are
installed.

\calloutitem{Standard Input} Ignored.

\calloutitem{Standard Output} JSON containing the following items:
\begin{itemize}
\item{\tt name} - A string serving as a short name for the archiver.
  This value should be lowercase and match the name of the directory
  where the archiver implementation is installed.  Archiver names
  should reflect the destination, such as {\tt bitbucket} for a data
  sink or {\tt esmond} for an archiving system like {\it esmond}.
\item{\tt description} - A short, textual description of what the
  archiver does.
\item{\tt version} - A \jsontype{Version}.
\item{\tt maintainer} - A \jsontype{Maintainer}.
\end{itemize}

\example
\begin{lstlisting}[language=json,firstnumber=1]
{
    "name": "bitbucket",
    "description": "Dump results into the bit bucket.",
    "version": "1.0",

    "maintainer": {
        "name": "perfSONAR Development Team",
        "email": "info@perfsonar.net",
        "href": "http://www.perfsonar.net"
    }
}
\end{lstlisting}

\calloutitem{Standard Error} Empty if successful, plain text error
messages on error.

\calloutitem{Exit Status} {\tt 0} if successful, nonzero on error.



\subsubsection{data-is-valid}

This method determines whether or not provided data is valid for use
by the archiver.  Note that this checks the data for validity only and
not for whether or not the data will result in the archiving being
successful.  It should not do anything that could be long-running or
may be intermittent such as performing DNS lookups or determining if a
web server answers.

\calloutitem{Standard Input} JSON containing the {\tt data} element of
an \jsontype{ArchiveSpecification}.  If the archive specification
contained no {\tt data} element, the standard input will be an empty
JSON object (i.e., {\tt \{ \}}).

\calloutitem{Standard Output} A JSON object containing the following pairs:

\todo{This gets used in multiple places and should be brought into the
  JSON dictionary.}
\begin{itemize}
\item \typeditem{valid}{Boolean} An indication of whether or not the
  input was valid.

\item \typeditem{error}{String} if {\tt valid} was \false, a plain
  text message indicating the reason.
\end{itemize}


\calloutitem{Standard Error} Empty if successful, plain text error
messages describing any errors other than the data being invald.

\calloutitem{Exit Status} {\tt 0} if successful, {\tt 1} if there was
any error unrelated to the input.


\subsubsection{archive}

This method sends the results of a run to the archiver.

\calloutitem{Standard Input} JSON containing the following items:


\begin{itemize}
\item \typeditem{data}{AnyJSON} Archiver-specific information on how to proceed.

\item \typeditem{result}{RunResult} A {\tt RunResult} describing
  the results of the run.

\item \typeditem{attempts}{Cardinal} The number of prior, unsuccessful
  attempts to archive the run.

\item \typeditem{last-attempt}{Timestamp} The time of the last attempt
  to archive the run.  This will be {\tt null} if there have been no
  prior attempts.
\end{itemize}

\example
\begin{lstlisting}[language=json,firstnumber=1]
{
    "data": { ...Archiver-Specific Data... },
    "task": { ...Task specification... },
    "result": { ...Results of run... },
    "attempts": 2,
    "last-attempt": "2016-05-07T13:05:27"
}
\end{lstlisting}


\calloutitem{Standard Output} A JSON object with information on how
the attempt to archive the run went, containing the following pairs:

\begin{itemize}
\item \typeditem{succeeded}{Boolean} Whether or not the attempt to archive
the run succeeded.
\item \typeditem{error}{String} An optional explanation of why the
  attempt to archive the run did not succeed.
\item \typeditem{retry}{Duration} The minimum amount of time that
  should elapse before the next attempt.  Not provided if {\tt
    succeeded} is \true\  or no further attempts should be made to
  archive the run.
\end{itemize}

\example
\begin{lstlisting}[language=json,firstnumber=1]
{
    "succeeded": false,
    "error": "Unable to connect: Unknown host 'badhost'",
    "retry": "PT15M"
}
\end{lstlisting}


\calloutitem{Standard Error} Empty if successful, plain text error
messages describing any problem unrelated to the input.

\calloutitem{Exit Status} {\tt 0} if successful, {\tt 1} if there was
an error.  Note that in this context, an error means a problem running
the method and not with the archiving process.




% -----------------------------------------------------------------------------

\part{Building Plug-Ins}

\section{Implementation}
\todo{Write this.}

\section{Packaging}

\subsection{Red Hat Enterprise Linux, CentOS and Fedora}
\todo{Write this.}

The RPM specs for the {\tt pscheduler-archiver-bitbucket}, {\tt
  pscheduler-test-idle} and {\tt pscheduler-tool-sleep} packages are
recommended to be used as templates for new development.

With {\tt pscheduler-rpm} installed on the system, the following
macros are available for use in RPM specifications:
\begin{center}
  \begin{tabular}{ll} 
    % Tests
    {\tt _pscheduler_test_libexec} & Test implementations \\
    {\tt _pscheduler_test_doc} & Test documentation \\
    % Tools
    {\tt _pscheduler_tool_libexec} & Tool implementations \\
    {\tt _pscheduler_tool_doc} & Tool documentation \\
    % Archivers
    {\tt _pscheduler_archiver_libexec} & Archiver implementations \\
    {\tt _pscheduler_archiver_doc} & Archiver documentation \\
  \end{tabular}
\end{center}

Test, tool and archiver implementations should be installed in a
subdirectory matching their names.  For example,
{\tt\%\{_pscheduler_tool_libexec\}/owamp} would be the recommended
way to specify the installation directory for the {\tt owamp} tool.

\subsection{Debian}
\todo{Write this after Antoine has had a chance to package a few
  things.}

\subsection{OS X}
\todo{Do we want to support OS X?  Nothing in pScheduler should be
  Linux-specific.  Some tools may need to understand what environment
  they're running in when invoking test programs because command line
  switches or other things may be different (e.g., With {\tt ping(8)},
  the switch to change TOS is {\tt -Q} where OS X uses {\tt -z}).
  This will either need to be accommodated by making the tools smart
  enough to detect and handle the differences or having multiple
  versions.  Either way, there may be a need for a list of supported
  OS names to be in the enumeration.  (Use the output of {\tt uname
    -s} for this.)}

\end{document}
