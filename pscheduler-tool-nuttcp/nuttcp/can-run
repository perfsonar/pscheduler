#!/usr/bin/env python3
#
# Determine if this tool can run a test based on a test spec.
#

import datetime
import sys

import pscheduler

logger = pscheduler.Log(prefix='tool-nuttcp', quiet=True)

json = pscheduler.json_load(exit_on_error=True)

try:
    if json['type'] != 'throughput':
        pscheduler.succeed_json({
            "can-run": False,
            "reasons": [ "Unsupported test type" ]
        })
except KeyError:
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": [ "Missing test type" ]
    })



errors = []

try:
    spec = json["spec"]
    pscheduler.json_check_schema(spec, 6)
except KeyError:
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": ["Missing test specification"]
    })
except ValueError as ex:
    pscheduler.succeed_json({
        "can-run": False,
        "reasons": [str(ex)]
    })

supported_options = [
    "bandwidth",
    "bandwidth-strict",
    "buffer-length",
    "burst-size",
    "client-cpu-affinity",
    "dest",
    "dest-node",
    "duration",
    "interval",
    "ip-tos",
    "ip-version",
    "mss",
    "parallel",
    "reverse",
    "reverse-connections",
    "schema",
    "server-cpu-affinity",
    "source",
    "single-ended",
    "single-ended-port",
    "source-node",
    "udp",
    "window-size",
    "loopback"
]

for option in spec:
    if option not in supported_options:
        logger.debug("nuttcp unsupported option %s" % option)
        errors.append('nuttcp does not support %s option' % option)

logger.debug("can-run succeeded")

result = {
    "can-run": len(errors) == 0
}

if len(errors) > 0:
    result["reasons"] = errors

pscheduler.succeed_json(result)
