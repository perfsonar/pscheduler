#!/usr/bin/python
#
# 
#

import datetime
import detach
import errno
import json
import optparse
import os
import pscheduler
import psycopg2
import psycopg2.extensions
import select
import socket
import sys
import time
import traceback

from dateutil.tz import tzlocal



# Gargle the arguments

opt_parser = optparse.OptionParser()
opt_parser.add_option("-c", "--channel",
                      help="Schedule notification channel",
                      action="store", type="string", dest="channel",
                      default="archiving_change")
# TODO: Do we want pscheduler as the default here?
opt_parser.add_option("-d", "--dsn",
                      help="Database connection string",
                      action="store", type="string", dest="dsn",
                      default="dbname=pscheduler")
opt_parser.add_option("-r", "--refresh",
                      help="Forced refresh interval (ISO8601)",
                      action="store", type="string", dest="refresh",
                      default="PT1M")
opt_parser.add_option("--verbose", action="store_true", dest="verbose")
opt_parser.add_option("--debug", action="store_true", dest="debug")


(options, args) = opt_parser.parse_args()

refresh = pscheduler.iso8601_as_timedelta(options.refresh)
if refresh is None:
    opt_parser.error('Invalid refresh interval "' + options.refresh + '"')
if pscheduler.timedelta_as_seconds(refresh) == 0:
    opt_parser.error("Refresh interval must be calculable as seconds.")

log = pscheduler.Log(verbose=options.verbose, debug=options.debug)

dsn = options.dsn



#
# Main Program
#

def main_program():

    # TODO: All DB transactions need to be error checked

    pg = pscheduler.pg_connection(dsn)
    cursor = pg.cursor()
    cursor.execute("LISTEN " + options.channel)

    # This cursor is for doing updates inside the other's loop.
    update_cursor = pg.cursor()


    next_refresh = None

    while True:

        # Wait for something to happen.

        if next_refresh is None:

            log.debug("Retrieving immediately.")

        else:

            log.debug("Waiting %s for change or notification", next_refresh)

            try:
                if select.select([pg],[],[],
                                 pscheduler.timedelta_as_seconds(next_refresh)) \
                                 != ([],[],[]):
                    pg.poll()
                    del pg.notifies[:]
                    log.debug("Notified.")

            except select.error as ex:

                err_no, message = ex
                if err_no != errno.EINTR:
                    log.exception()
                    raise ex


        # Until we hear otherwise...
        next_refresh = refresh

        cursor.execute("""SELECT id, uuid, archiver, archiver_data, start,
                      duration, test, tool, participants, result_full, result,
                      attempts, last_attempt
                      FROM archiving_eligible""")

        if cursor.rowcount == 0:
            log.debug("Nothing to archive.")
            continue



        for row in cursor.fetchall():

            run_id, uuid, archiver, archiver_data, start, duration, test, \
                tool, participants, result_full, result_merged, attempts, \
                last_attempt = row

            assert len(participants) == len(result_full)
            participants_merged = []
            for participant in participants:
                participants_merged.append({
                        'host': socket.gethostname() if participant is None \
                            else participant,
                        'result': result_full.pop(0)
                        })


            json = {
                'data': archiver_data,
                'result': {
                    'id': uuid,
                    'schedule': {
                        'start': pscheduler.datetime_as_iso8601(start),
                        'duration': pscheduler.timedelta_as_iso8601(duration)
                        },
                    'test': test,
                    'tool': {
                        'name': tool['name'],
                        'verion': tool['version'],
                        },
                    'participants': participants_merged,
                    'result': result_merged
                    },
                'attempts': attempts,
                'last-attempt': None if last_attempt is None \
                    else pscheduler.datetime_as_iso8601(last_attempt) 
                }


            archiver_in = pscheduler.json_dump(json)
            log.debug("Running archiver %s with input %s", archiver, archiver_in)

            returncode, stdout, stderr = pscheduler.run_program(
                [ "pscheduler", "internal", "invoke", "archiver", archiver, "archive" ],
                stdin = archiver_in,
                timeout = 10  # TODO: What's appropriate here?
                )

            log.debug("Archiver exited %d", returncode)
            log.debug("Returned JSON from archiver: %s", stdout)
            log.debug("Returned errors from archiver: %s", stderr)

            attempt = pscheduler.json_dump( [ {
                        "time": pscheduler.datetime_as_iso8601(datetime.datetime.now(tzlocal())),
                        "return-code": returncode,
                        "stdout": stdout,
                        "stderr": stderr
                        } ] )



            if returncode != 0:

                log.debug("Permanent Failure: %s", stderr)
                update_cursor.execute("""UPDATE archiving
                                     SET
                                       archived = TRUE,
                                       attempts = attempts + 1,
                                       last_attempt = now(),
                                       next_attempt = NULL,
                                       diags = diags || (%s::JSONB)
                                     WHERE id = %s""",
                                      [ attempt, run_id ])
                pass  # TODO: Update with diags

            else:


                try:
                    returned_json = pscheduler.json_load(stdout)
                except ValueError:
                    log.error("Archiver %s returned invalid JSON: %s", archiver, stdout)
                    continue

                if returned_json['succeeded']:
                    log.debug("Succeeded: %s to %s", uuid, archiver)
                    update_cursor.execute("""UPDATE archiving
                                         SET
                                             archived = TRUE,
                                             attempts = attempts + 1,
                                             last_attempt = now(),
                                             next_attempt = NULL,
                                             diags = diags || (%s::JSONB)
                                         WHERE id = %s""",
                                          [ attempt, run_id ])
                else:
                    log.debug("Failed: %s to %s: %s", uuid, archiver, stdout)
                    next_delta = pscheduler.iso8601_as_timedelta(
                        returned_json['retry'])

                    next = datetime.datetime.now(tzlocal()) \
                        + next_delta

                    update_cursor.execute("""UPDATE archiving
                                         SET
                                             attempts = attempts + 1,
                                             last_attempt = now(),
                                             next_attempt = %s,
                                             diags = diags || (%s::JSONB)
                                         WHERE id = %s""",
                                          [ next, attempt, run_id ])

                    if next_delta < next_refresh:
                        next_refresh = next_delta


pscheduler.safe_run(lambda: main_program())
